{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from itertools import dropwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=30\n",
    "path_to_file1 = 'combinedChatQuestionAnswerFormatDelearCust'\n",
    "path_to_file2 = 'combinedChatQuestionAnswerFormatCustDelear'\n",
    "path_to_inp_lang = 'inp_lang'\n",
    "path_to_targ_lang = 'targ_lang'\n",
    "WORDS_FREQ_THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "     #Remove URLS\n",
    "    w = re.sub(r\"http\\S+\", \"\", w)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(paths, num_examples):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for path in paths:\n",
    "        lines = []\n",
    "        with open (path, 'rb') as fp:\n",
    "            lines = pickle.load(fp)\n",
    "        for i in range(0, len(lines), 2):\n",
    "            questions.append(preprocess_sentence(lines[i]))\n",
    "            answers.append(preprocess_sentence(lines[i+1]))\n",
    "        \n",
    "    return (questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsWithFreqLessThanK(wordCounter, k):\n",
    "    \n",
    "    cnt = 0\n",
    "    for word in wordCounter:\n",
    "        if wordCounter[word] < k:\n",
    "            cnt += 1\n",
    "            #print(word, wordCounter[word])\n",
    "            \n",
    "    print(\"total \", cnt, \" out of \", len(wordCounter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWordsWithFreqLessThanK(wordCounter, k):\n",
    "    \n",
    "    cnt = 0\n",
    "    for word in wordCounter:\n",
    "        if wordCounter[word] < k:\n",
    "            cnt += 1\n",
    "            #del wordCounter[word]\n",
    "            #print(word, wordCounter[word])\n",
    "            \n",
    "    print(\"removing \", cnt, \" out of \", len(wordCounter), len(wordCounter)- cnt)\n",
    "    \n",
    "    for key, count in dropwhile(lambda key_count: key_count[1] >= k, wordCounter.most_common()):\n",
    "        del wordCounter[key]\n",
    "    \n",
    "    print(\"final count: \", len(wordCounter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "  def __init__(self, lang):\n",
    "    self.lang = lang\n",
    "    self.word2idx = {}\n",
    "    self.idx2word = {}\n",
    "    self.vocab = set()\n",
    "    self.wordCounter = {}\n",
    "    \n",
    "    self.create_index()\n",
    "    \n",
    "  def create_index(self):\n",
    "#     for phrase in self.lang:\n",
    "#       self.vocab.update(phrase.split(' '))\n",
    "    \n",
    "#     self.vocab = sorted(self.vocab)\n",
    "    \n",
    "    self.wordCounter = Counter(chain.from_iterable(map(lambda x: x.split(' '), self.lang)))\n",
    "    removeWordsWithFreqLessThanK(self.wordCounter, WORDS_FREQ_THRESHOLD)\n",
    "    \n",
    "    self.vocab = sorted(set(list(self.wordCounter.keys())))\n",
    "    self.word2idx['<pad>'] = 0\n",
    "    self.word2idx['<unk>'] = 1\n",
    "    for index, word in enumerate(self.vocab):\n",
    "      self.word2idx[word] = index + 2\n",
    "    \n",
    "    for word, index in self.word2idx.items():\n",
    "      self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = create_dataset([path_to_file1,path_to_file2],0)\n",
    "# inp_lang = LanguageIndex(pairs[0])\n",
    "# targ_lang = LanguageIndex(pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleLangobj(path, obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLangObj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_lang.wordCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from itertools import chain\n",
    "\n",
    "# list1=['hello apple','hello egg','hello2 apple','hello2 banana','hello3 egg','hello3 apple']\n",
    "# counts = Counter(chain.from_iterable(map(lambda x: x.split(' '), list1)))\n",
    "# print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    #return max(len(t) for t in tensor)\n",
    "    return MAX_SEQUENCE_LENGTH\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(pairs[0])\n",
    "    targ_lang = LanguageIndex(pairs[1])\n",
    "    \n",
    "    pickleLangobj(path_to_inp_lang,inp_lang)\n",
    "    pickleLangobj(path_to_targ_lang,targ_lang)\n",
    "\n",
    "    #print(inp_lang.word2idx[s] for s in pairs[0][0])\n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # Spanish sentences\n",
    "    #input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for sp in pairs[0]]\n",
    "    input_tensor = []\n",
    "    for sp in pairs[0]:\n",
    "        wordIds = []\n",
    "        for s in sp.split(' '):\n",
    "            if (s in inp_lang.word2idx):\n",
    "                wordIds.append(inp_lang.word2idx[s])\n",
    "            else:\n",
    "                wordIds.append(inp_lang.word2idx['<unk>'])\n",
    "        input_tensor.append(wordIds)\n",
    "    \n",
    "    # English sentences\n",
    "    #target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en in pairs[1]]\n",
    "    target_tensor = []\n",
    "    for sp in pairs[1]:\n",
    "        wordIds = []\n",
    "        for s in sp.split(' '):\n",
    "            if (s in targ_lang.word2idx):\n",
    "                wordIds.append(targ_lang.word2idx[s])\n",
    "            else:\n",
    "                wordIds.append(targ_lang.word2idx['<unk>'])\n",
    "        target_tensor.append(wordIds)\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = None\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset([path_to_file1,path_to_file2], num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_lang.wordCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 32#64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 512 #1024\n",
    "MAX_GRADIENT_VAL = 10.0\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical jargon H-> Hidden state1, C-> Hidden state2, both hidden states of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer notes\n",
    "- We declare the LSTM layers to process the input sentences. Layer has the \"units\"(this is units parameter in tf.keras.layers.CuDNNLSTM) number of LSTM cells chained. \n",
    "- This layer takes inputs of shape (batch size, max sequence length, embeddings dimensions). \n",
    "- This same layer is unfolded max sequence length times during the training. Same layer is used during each time step, the only difference in each time step is, it takes the input from previous time step and current input. This is the recurrent part and weights are shared accorss the training.  \n",
    "- For Unidirectinal:\n",
    "  - It returns output, hiddenstate1(H), hiddenstate2(C).\n",
    "  - Hidden states have shape -> (batchsize, LSTM Units).\n",
    "  - If return_sequences is set, output has the shape (batch_size, max_length, LSTM units), else it returns (batchsize, encoder units) only last step output.\n",
    "- For Bidirectinal :\n",
    "  - It returns both forwad and backward hidden states and concatinated output.OP, forward H, forward C, backward H, backward C.\n",
    "  - Hidden states have shape -> (batchsize, LSTM Units).\n",
    "  - If return_sequences is set, output has the shape (batch_size, max_length, 2xLSTM units), else it returns (batchsize, 2xLSTM units) only last step output.\n",
    "  - The x2 factor in return sizes corresponds to the bi-directinal cells.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    lstm = tf.keras.layers.CuDNNLSTM(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "#     lstm = tf.nn.rnn_cell.DropoutWrapper(lstm,input_keep_prob=0.8,\n",
    "# output_keep_prob=0.8,\n",
    "# state_keep_prob=1.0)\n",
    "\n",
    "    return tf.keras.layers.Bidirectional(lstm)\n",
    "  else:\n",
    "    return \"No GPU!!!\"    \n",
    "    #return tf.keras.layers.GRU(units, \n",
    "    #                           return_sequences=True, \n",
    "    ##                           return_state=True, \n",
    "    #                           recurrent_activation='sigmoid', \n",
    "     #                          recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    lstm = tf.keras.layers.CuDNNLSTM(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "#     lstm = tf.nn.rnn_cell.DropoutWrapper(lstm,input_keep_prob=0.8,\n",
    "#             output_keep_prob=0.8,\n",
    "#             state_keep_prob=1.0)\n",
    "    \n",
    "    return lstm\n",
    "  else:\n",
    "    return \"No GPU!!!\"    \n",
    "    #return tf.keras.layers.GRU(units, \n",
    "    #                           return_sequences=True, \n",
    "    ##                           return_state=True, \n",
    "    #                           recurrent_activation='sigmoid', \n",
    "     #                          recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = BiLSTM(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        #X shape is [batchsize, maxSeqLen]\n",
    "        x = self.embedding(x)\n",
    "        #X shape is [batchsize, maxSeqLen, embedding dimension]\n",
    "        output, fw_H, fw_C, bw_H, bw_C = self.bilstm(x)\n",
    "        #output, fw_H, fw_C, bw_H, bw_C = self.bilstm(x, initial_state = hidden)\n",
    "        \n",
    "        #Output shape is [batchsize, maxSeqLen, 2*LSTM Units]\n",
    "        #fw or bw hidden state shape is [batchsize, LSTM Units]\n",
    "        \n",
    "        ## We concatinate these states because in decoder we can only have a unidirectinal layer. This is because in \n",
    "        ## decoder we dont the future words.\n",
    "        final_H = tf.concat((fw_H, bw_H), 1)\n",
    "        #After concatenation hidden state shape is [batchsize, 2*LSTM Units]\n",
    "        final_C = tf.concat((fw_C, bw_C), 1)\n",
    "        return output, final_H, final_C\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units*2\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.dec_units)\n",
    "        ## We use this layer to pass the LSTM output to get the logits for each word, hence the dimension - vocab_size. \n",
    "        ## These logits are for each word being the next word in output sentence.\n",
    "        ## We pass this fc out to error function which takes logits as input.\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W3 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, dec_H, dec_C, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, 2*encoder units)\n",
    "        \n",
    "        \n",
    "        # hidden shape == (batch_size, decoder units)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size or decoder units)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        dec_H_time_axis = tf.expand_dims(dec_H, 1)\n",
    "        dec_C_time_axis = tf.expand_dims(dec_C, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H) +FC(C)) to self.V\n",
    "        # The out put of a lstm layer is not a softmax(not logits, so pass them through dense layers)\n",
    "        # and get the Logits and then pass to a dense layer.\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(dec_H_time_axis)+ self.W3(dec_C_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, H, C = self.lstm(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, H, C, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'D:\\\\training_checkpoints2'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 35\n",
    "\n",
    "startEpoch = 0\n",
    "\n",
    "try:\n",
    "    with open(checkpoint_dir+'\\\\epochNum', 'rb') as f:\n",
    "        startEpoch = pickle.load(f)\n",
    "    print('starting with epoch', startEpoch)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "except:\n",
    "    print('start epoch not found')\n",
    "    \n",
    "for epoch in range(startEpoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden_H, enc_hidden_C = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden_H = enc_hidden_H\n",
    "            dec_hidden_C = enc_hidden_C\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden_H, dec_hidden_C, _ = decoder(dec_input, dec_hidden_H, dec_hidden_C, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        ## Tapes together all the variables for which we needs to compute the gradients. Here embeddings, dense layers and \n",
    "        ## LSTM's weights will be taped together for gradient calculations.\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        ## This is to clip the gradients. We might run into the exploding gradients problem while traing the model.\n",
    "        ## To avoid this we use clipping.\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, MAX_GRADIENT_VAL)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        with open(checkpoint_dir+'\\\\epochNum', 'wb') as f:\n",
    "            pickle.dump(epoch+1, f)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resumeTraining(dataset):\n",
    "    \n",
    "#     checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "#     EPOCHS = 30\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         start = time.time()\n",
    "\n",
    "#         hidden = encoder.initialize_hidden_state()\n",
    "#         total_loss = 0\n",
    "\n",
    "#         for (batch, (inp, targ)) in enumerate(dataset):\n",
    "#             loss = 0\n",
    "\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 enc_output, enc_hidden_H, enc_hidden_C = encoder(inp, hidden)\n",
    "\n",
    "#                 dec_hidden_H = enc_hidden_H\n",
    "#                 dec_hidden_C = enc_hidden_C\n",
    "\n",
    "#                 dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "\n",
    "#                 # Teacher forcing - feeding the target as the next input\n",
    "#                 for t in range(1, targ.shape[1]):\n",
    "#                     # passing enc_output to the decoder\n",
    "#                     predictions, dec_hidden_H, dec_hidden_C, _ = decoder(dec_input, dec_hidden_H, dec_hidden_C, enc_output)\n",
    "\n",
    "#                     loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "#                     # using teacher forcing\n",
    "#                     dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "#             batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "#             total_loss += batch_loss\n",
    "\n",
    "#             variables = encoder.variables + decoder.variables\n",
    "\n",
    "#             gradients = tape.gradient(loss, variables)\n",
    "\n",
    "#             gradients, _ = tf.clip_by_global_norm(gradients, MAX_GRADIENT_VAL)\n",
    "\n",
    "#             optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "#             if batch % 100 == 0:\n",
    "#                 print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                                              batch,\n",
    "#                                                              batch_loss.numpy()))\n",
    "#         # saving (checkpoint) the model every 2 epochs\n",
    "#         if (epoch + 1) % 2 == 0:\n",
    "#           checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "#         print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                             total_loss / N_BATCH))\n",
    "#         print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeTraining(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    \n",
    "    #inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = []\n",
    "    for i in sentence.split(' '):\n",
    "        if(i in inp_lang.word2idx):\n",
    "            inputs.append(inp_lang.word2idx[i])\n",
    "        else:\n",
    "            inputs.append(inp_lang.word2idx['<unk>'])\n",
    "    \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_output, enc_hidden_H, enc_hidden_C = encoder(inputs, hidden)\n",
    "    dec_hidden_H = enc_hidden_H\n",
    "    dec_hidden_C = enc_hidden_C\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_H, dec_hidden_C, attention_weights = decoder(dec_input, dec_hidden_H, dec_hidden_C, enc_output)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Hello how are you?', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
